<!-- HTML header for doxygen 1.13.2-->
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
  <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=11" />
  <meta name="generator" content="Doxygen 1.16.1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NEML2: Statistical material model learning</title>
  <link href="tabs.css" rel="stylesheet" type="text/css" />
  <script type="text/javascript" src="jquery.js"></script>
  <script type="text/javascript" src="dynsections.js"></script>
  <script type="text/javascript" src="clipboard.js"></script>
  <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
  <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
  <script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  }
};
window.MathJax = {
  loader: {load: ['[tex]/ams', '[tex]/physics', '[tex]/boldsymbol']},
  tex: {packages: {'[+]': ['ams', 'physics', 'boldsymbol']},
        tags: 'ams'}
};
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <link href="doxygen.css" rel="stylesheet" type="text/css" />
  <link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-tabs.js" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
  <script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
  <script type="text/javascript" src="doxygen-awesome-paragraph-link.js"></script>
  <script type="text/javascript" src="doxygen-awesome-tabs.js"></script>
  <script type="text/javascript" src="doxygen-awesome-interactive-toc.js"></script>
  <!-- Demo animations -->
  <script type="text/javascript" src="anime.min.js"></script>
  <script type="text/javascript" src="work-dispatcher.js"></script>
  <script type="text/javascript" src="static-hybrid-scheduler.js"></script>
  <script type="text/javascript" src="simple-scheduler-demo.js"></script>
  <script type="text/javascript" src="static-hybrid-scheduler-demo.js"></script>
  <script type="text/javascript">
    DoxygenAwesomeDarkModeToggle.init();
    DoxygenAwesomeParagraphLink.init();
    DoxygenAwesomeInteractiveToc.init();
    DoxygenAwesomeTabs.init();
    SimpleSchedulerDemo.init();
    StaticHybridSchedulerDemo.init();
  </script>
</head>
<body>
    <div id="top"><!-- do not remove this div, it is closed by doxygen! -->
      <div id="titlearea">
        <table cellspacing="0" cellpadding="0">
          <tbody>
            <tr id="projectrow">
              <td id="projectalign">
                <div id="projectname">NEML2<span
                    id="projectnumber">&#160;2.0.0</span>
                </div>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <!-- end header part -->
<!-- Generated by Doxygen 1.16.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('python-examples-statistical.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Statistical material model learning </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_build_2doc_2content_2tutorials_2python_2examples_2statistical"></a></p>
<p><b>This is a modification of the deterministic example to use Stochastic Variational Inference (SVI) to train a hierarchical statistical model rather than a simple deterministic model. The modifications required to train a model with SVI only start half way through the example &mdash; the rest is the same as the previous, deterministic example.</b></p>
<p>This is a complete example of using the pyzag bindings to NEML2 to calibrated a material model against experimental data. <span class="tt">demo_model.i</span> defines the constitutive model, which is a structural material model describing the evolution of strain and stress in the material under mechanical load. The particular demonstration is a fairly complex model where the material responds differently as a function of both temperature and strain rate. The model also reduces the full 3D form of the constitutive model to 1D, where the final model is driven by an axial strain and all the other stress components are zero.</p>
<p>In this example we:</p><ol type="1">
<li>Load the model in from file and wrap it for use in pyzag</li>
<li>Setup a grid of "experimental" conditions spanning several strain rates and temperatures</li>
<li>Replace the original model parameters with samples from a normal distrubtion, centered on the orignial model mean, and run the model over the experimental conditions. This then becomes our synthetic input data.</li>
<li>Replace the original model parameters with random priors from wide normal distributions (with means sampled from the original distributions).</li>
<li>Setup the model for training with gradient-descent methods by scaling the model parameters and resulting gradient values.</li>
<li>Use SVI to train the model against the synthetic data.</li>
<li>Plot the results and print the trained parameter values, to see how close we can come to the true values.</li>
</ol>
<p>Because it's difficult to consider random variation across a wide range of test conditions, we also consider a set of repeated experiments at the same test condition. Simulating the same test many times gives a clear idea of the amount of variability in the synthetic data and for the corresponding trained model.</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> dist</div>
<div class="line"><span class="keyword">import</span> neml2</div>
<div class="line"><span class="keyword">from</span> pyzag <span class="keyword">import</span> nonlinear, reparametrization, chunktime, stochastic</div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line"><span class="keyword">import</span> tqdm</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> pyro</div>
<div class="line"><span class="keyword">from</span> pyro.infer <span class="keyword">import</span> SVI, Trace_ELBO, Predictive</div>
</div><!-- fragment --> <pre class="fragment">/home/gary/mambaforge3/envs/neml2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre><h1 class="doxsection"><a class="anchor" id="autotoc_md186"></a>
Setup parameters related to <em>how</em> we train the model</h1>
<p>Choose which device to use. The <span class="tt">nchunk</span> parameter controls the time integration in <span class="tt">pyzag</span>. <span class="tt">pyzag</span> can vectorize the time integration itself, providing a larger bandwidth to the compute device. This helps speed up the calculation, particularly when running on a GPU. The optimal value will depend on your compute device.</p>
<div class="fragment"><div class="line">torch.manual_seed(42)</div>
<div class="line"> </div>
<div class="line">torch.set_default_dtype(torch.double)</div>
<div class="line"><span class="keywordflow">if</span> torch.cuda.is_available():</div>
<div class="line">    dev = <span class="stringliteral">&quot;cuda:0&quot;</span></div>
<div class="line"><span class="keywordflow">else</span>:</div>
<div class="line">    dev = <span class="stringliteral">&quot;cpu&quot;</span></div>
<div class="line">device = torch.device(dev)</div>
<div class="line"> </div>
<div class="line">nchunk = 100</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md187"></a>
Setup the synthetic experimental conditions</h1>
<p>Setup the loading conditions for the "experiments" we're going to run. These will span several strain rates (<span class="tt">nrate</span>) and temperatures (<span class="tt">ntemperature</span>). Overall, we'll run <span class="tt">nbatch</span> experiments. Also setup the maximum strain to pull the material through <span class="tt">max_strain</span> and the number of time steps we're going to use for integration <span class="tt">ntime</span>.</p>
<div class="fragment"><div class="line">nrate = 5</div>
<div class="line">ntemperature = 10</div>
<div class="line">nbatch = nrate * ntemperature</div>
<div class="line">max_strain = 0.25</div>
<div class="line">ntime = 100</div>
<div class="line">rates = torch.logspace(-6, 0, nrate, device=device)</div>
<div class="line">temperatures = torch.linspace(310.0, 1190.0, ntemperature, device=device)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md188"></a>
Conditions for the repeated experiment</h1>
<p>Also choose a single condition to repeat multiple times to give a clearer idea of the variability captured in the synthetic data and model.</p>
<div class="fragment"><div class="line">nrepeat = 100</div>
<div class="line">single_temperature = 600.0</div>
<div class="line">single_rate = 1.0e-3</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md189"></a>
Define the variability in the synthetic data and for our initial guess at the parameters</h1>
<p>These control the variability in the synthetic data (<span class="tt">actual_cov</span>) and the variability of the initial guess at the parameter values (<span class="tt">guess_cov</span>). Also provide the actual values and a prior on the white noise included on top of the experimental measurements.</p>
<div class="fragment"><div class="line">actual_cov = 0.02</div>
<div class="line">actual_noise_scale = 5.0</div>
<div class="line">prior_cov = 0.05</div>
<div class="line">prior_noise_scale = 10.0</div>
<div class="line"> </div>
<div class="line">guess_cov = 0.1  <span class="comment"># On the means of the priors</span></div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md190"></a>
Setup the actual model</h1>
<p>This class is a thin wrapper around the underlying <span class="tt">pyzag</span> wrapper for NEML2. All it does is take the input conditions (time, temperature, and strain), combine them into a single tensor, call the <span class="tt">pyzag</span> wrapper, and return the stress.</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>SolveStrain(torch.nn.Module):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Just integrate the model through some strain history</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">    Args:</span></div>
<div class="line"><span class="stringliteral">        discrete_equations: the pyzag wrapped model</span></div>
<div class="line"><span class="stringliteral">        nchunk (int): number of vectorized time steps</span></div>
<div class="line"><span class="stringliteral">        rtol (float): relative tolerance to use for Newton&#39;s method during time integration</span></div>
<div class="line"><span class="stringliteral">        atol (float): absolute tolerance to use for Newton&#39;s method during time integration</span></div>
<div class="line"><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>__init__(self, discrete_equations, nchunk=1, rtol=1.0e-6, atol=1.0e-4):</div>
<div class="line">        super().__init__()</div>
<div class="line">        self.discrete_equations = discrete_equations</div>
<div class="line">        self.nchunk = nchunk</div>
<div class="line">        self.cached_solution = <span class="keywordtype">None</span></div>
<div class="line">        self.rtol = rtol</div>
<div class="line">        self.atol = atol</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>forward(self, time, temperature, loading, cache=False):</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;Integrate through some time/temperature/strain history and return stress</span></div>
<div class="line"><span class="stringliteral">        Args:</span></div>
<div class="line"><span class="stringliteral">            time (torch.tensor): batched times</span></div>
<div class="line"><span class="stringliteral">            temperature (torch.tensor): batched temperatures</span></div>
<div class="line"><span class="stringliteral">            loading (torch.tensor): loading conditions, which are the input strain in the first base index and then the stress (zero) in the remainder</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">        Keyword Args:</span></div>
<div class="line"><span class="stringliteral">            cache (bool): if true, cache the solution and use it as a predictor for the next call.</span></div>
<div class="line"><span class="stringliteral">                This heuristic can speed things up during inference where the model is called repeatedly with similar parameter values.</span></div>
<div class="line"><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line">        <span class="keywordflow">if</span> cache <span class="keywordflow">and</span> self.cached_solution <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line">            solver = nonlinear.RecursiveNonlinearEquationSolver(</div>
<div class="line">                self.discrete_equations,</div>
<div class="line">                step_generator=nonlinear.StepGenerator(self.nchunk),</div>
<div class="line">                predictor=nonlinear.FullTrajectoryPredictor(self.cached_solution),</div>
<div class="line">                nonlinear_solver=chunktime.ChunkNewtonRaphson(rtol=self.rtol, atol=self.atol),</div>
<div class="line">            )</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            solver = nonlinear.RecursiveNonlinearEquationSolver(</div>
<div class="line">                self.discrete_equations,</div>
<div class="line">                step_generator=nonlinear.StepGenerator(self.nchunk),</div>
<div class="line">                predictor=nonlinear.PreviousStepsPredictor(),</div>
<div class="line">                nonlinear_solver=chunktime.ChunkNewtonRaphson(rtol=self.rtol, atol=self.atol),</div>
<div class="line">            )</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># We could pass this in as input, but it&#39;s easy enough to do here</span></div>
<div class="line">        control = torch.zeros_like(loading)</div>
<div class="line">        control[..., 1:] = 1.0</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Setup</span></div>
<div class="line">        forces = {</div>
<div class="line">            <span class="stringliteral">&quot;forces/t&quot;</span>: <a class="code hl_class" href="classneml2_1_1Scalar.html">neml2.Scalar</a>(time.squeeze(-1), 0),</div>
<div class="line">            <span class="stringliteral">&quot;forces/T&quot;</span>: <a class="code hl_class" href="classneml2_1_1Scalar.html">neml2.Scalar</a>(temperature.squeeze(-1), 0),</div>
<div class="line">            <span class="stringliteral">&quot;forces/fixed_values&quot;</span>: <a class="code hl_class" href="classneml2_1_1SR2.html">neml2.SR2</a>(loading, 0),</div>
<div class="line">            <span class="stringliteral">&quot;forces/control&quot;</span>: <a class="code hl_class" href="classneml2_1_1SR2.html">neml2.SR2</a>(control, 0),</div>
<div class="line">        }</div>
<div class="line">        forces = [forces[key] <span class="keywordflow">for</span> key <span class="keywordflow">in</span> self.discrete_equations.fmap]</div>
<div class="line">        forces = neml2.assemble_vector(forces, self.discrete_equations.flayout).<a class="code hl_namespace" href="namespacetorch.html">torch</a>()</div>
<div class="line">        state0 = torch.zeros(</div>
<div class="line">            forces.shape[1:-1] + (self.discrete_equations.nstate,), device=forces.device</div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        result = nonlinear.solve_adjoint(solver, state0, len(forces), forces)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> cache:</div>
<div class="line">            self.cached_solution = result.detach().clone()</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">return</span> result[..., 0:1]</div>
<div class="ttc" id="aclassneml2_1_1SR2_html"><div class="ttname"><a href="classneml2_1_1SR2.html">neml2::SR2</a></div><div class="ttdoc">The symmetric second order tensor.</div><div class="ttdef"><b>Definition</b> SR2.h:46</div></div>
<div class="ttc" id="aclassneml2_1_1Scalar_html"><div class="ttname"><a href="classneml2_1_1Scalar.html">neml2::Scalar</a></div><div class="ttdoc">Scalar.</div><div class="ttdef"><b>Definition</b> Scalar.h:38</div></div>
<div class="ttc" id="anamespacetorch_html"><div class="ttname"><a href="namespacetorch.html">torch</a></div><div class="ttdef"><b>Definition</b> TransientDriver.h:32</div></div>
</div><!-- fragment --><h2 class="doxsection"><a class="anchor" id="autotoc_md191"></a>
Actually setup the model</h2>
<p>Load the NEML model from disk, wrap it in both the <span class="tt">pyzag</span> wrapper and our thin wrapper class above. Exclude some of the model parameters we don't want to train.</p>
<div class="fragment"><div class="line">nmodel = neml2.load_nonlinear_system(<span class="stringliteral">&quot;demo_model.i&quot;</span>, <span class="stringliteral">&quot;eq_sys&quot;</span>)</div>
<div class="line">nmodel.to(device=device)</div>
<div class="line">pmodel = neml2.pyzag.NEML2PyzagModel(</div>
<div class="line">    nmodel,</div>
<div class="line">    exclude_parameters=[</div>
<div class="line">        <span class="stringliteral">&quot;elasticity_E&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;elasticity_nu&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;R_X&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;d_X&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;mu_X&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;mu_Y&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;yield_zero_sy&quot;</span>,</div>
<div class="line">    ],</div>
<div class="line">)</div>
<div class="line">model = SolveStrain(pmodel)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md192"></a>
Create the input tendors</h1>
<p>Actually setup the full input tensors based on the parameters above</p>
<div class="fragment"><div class="line">time = torch.zeros((ntime, nrate, ntemperature), device=device)</div>
<div class="line">loading = torch.zeros((ntime, nrate, ntemperature, 6), device=device)</div>
<div class="line">temperature = torch.zeros((ntime, nrate, ntemperature), device=device)</div>
<div class="line"><span class="keywordflow">for</span> i, rate <span class="keywordflow">in</span> enumerate(rates):</div>
<div class="line">    time[:, i] = torch.linspace(0, max_strain / rate, ntime, device=device)[:, <span class="keywordtype">None</span>]</div>
<div class="line">loading[..., 0] = torch.linspace(0, max_strain, ntime, device=device)[:, <span class="keywordtype">None</span>, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="keywordflow">for</span> i, T <span class="keywordflow">in</span> enumerate(temperatures):</div>
<div class="line">    temperature[:, :, i] = T</div>
<div class="line">time = time.reshape((ntime, -1, 1))</div>
<div class="line">temperature = temperature.reshape((ntime, -1, 1))</div>
<div class="line">loading = loading.reshape((ntime, -1, 6))</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">single_times = (</div>
<div class="line">    torch.linspace(0, max_strain / single_rate, ntime, device=device)</div>
<div class="line">    .unsqueeze(-1)</div>
<div class="line">    .expand((ntime, nrepeat))</div>
<div class="line">    .unsqueeze(-1)</div>
<div class="line">)</div>
<div class="line">single_temperatures = torch.full_like(single_times, single_temperature)</div>
<div class="line">single_loading = torch.zeros((ntime, nrepeat, 6), device=device)</div>
<div class="line">single_loading[..., 0] = torch.linspace(0, max_strain, ntime, device=device).unsqueeze(-1)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md193"></a>
Replace the model parameters with random values</h1>
<p>Sampled from a normal distribution controlled by the <span class="tt">actual_cov</span> parameter.</p>
<p>This controls the randomness in the input synthetic test data</p>
<div class="fragment"><div class="line"><span class="comment"># Replace with samples from normal</span></div>
<div class="line">actual_parameter_values = {}</div>
<div class="line"><span class="keywordflow">for</span> n, p <span class="keywordflow">in</span> model.named_parameters():</div>
<div class="line">    actual_parameter_values[n] = p.data.detach().clone().cpu()</div>
<div class="line">    ndist = dist.Normal(p.data, torch.abs(p.data) * actual_cov).expand((nbatch,) + p.shape)</div>
<div class="line">    p.data = ndist.sample().to(device)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md194"></a>
Run the model to generate the synthetic data</h1>
<div class="fragment"><div class="line"><span class="keyword">with</span> torch.no_grad():</div>
<div class="line">    data = model(time, temperature, loading)</div>
<div class="line">    data = torch.normal(data, actual_noise_scale)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md195"></a>
Plot the synthetic data</h1>
<div class="fragment"><div class="line">plt.figure()</div>
<div class="line">plt.plot(loading.cpu()[..., 0], data[..., 0].cpu())</div>
<div class="line">plt.xlabel(<span class="stringliteral">&quot;Strain (mm/mm)&quot;</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&quot;Stress (MPa)&quot;</span>)</div>
<div class="line">plt.title(<span class="stringliteral">&quot;Input data -- all conditions&quot;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --><div class="image">
<img src="statistical_21_0.png" alt=""/>
<div class="caption">
png</div></div>
    <h1 class="doxsection"><a class="anchor" id="autotoc_md196"></a>
Now sample at run at a fixed condition</h1>
<p>The idea being to clearly see the variability in repeated trials at a fix condition.</p>
<div class="fragment"><div class="line"><span class="comment"># Replace with samples from normal</span></div>
<div class="line"><span class="keywordflow">for</span> n, p <span class="keywordflow">in</span> model.named_parameters():</div>
<div class="line">    ndist = dist.Normal(</div>
<div class="line">        actual_parameter_values[n], torch.abs(actual_parameter_values[n]) * actual_cov</div>
<div class="line">    ).expand((nrepeat,) + actual_parameter_values[n].shape)</div>
<div class="line">    p.data = ndist.sample().to(device)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">with</span> torch.no_grad():</div>
<div class="line">    single_data = model(single_times, single_temperatures, single_loading)</div>
<div class="line">    single_data = torch.normal(single_data, actual_noise_scale)</div>
<div class="line"> </div>
<div class="line">plt.figure()</div>
<div class="line">plt.plot(single_loading.cpu()[..., 0], single_data[..., 0].cpu())</div>
<div class="line">plt.xlabel(<span class="stringliteral">&quot;Strain (mm/mm)&quot;</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&quot;Stress (MPa)&quot;</span>)</div>
<div class="line">plt.title(<span class="stringliteral">&quot;Input data -- single condition&quot;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --><div class="image">
<img src="statistical_23_0.png" alt=""/>
<div class="caption">
png</div></div>
    <h1 class="doxsection"><a class="anchor" id="autotoc_md197"></a>
Setup the model for training</h1>
<p>Replace the parameter values with random initial guesses, with variability controlled by the <span class="tt">guess_cov</span> parameter.</p>
<div class="fragment"><div class="line"><span class="comment"># Now replace our original parameter with random values over a range</span></div>
<div class="line">guess_parameter_values = {}</div>
<div class="line"><span class="keywordflow">for</span> n, p <span class="keywordflow">in</span> model.named_parameters():</div>
<div class="line">    p.data = torch.normal(</div>
<div class="line">        actual_parameter_values[n], torch.abs(actual_parameter_values[n]) * guess_cov</div>
<div class="line">    ).to(device)</div>
<div class="line">    guess_parameter_values[n] = p.data.detach().clone()</div>
<div class="line">model.discrete_equations._update_parameter_values()</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md198"></a>
Scale the model parameters</h1>
<p>Our material model parameters have units. In general, the parameter values will have different magnitudes from each other, which affects the scale of the gradients. Unbalanced gradients in turn affect the convergence of gradient descent optimization methods.</p>
<p>Typically we'd scale the training data to fix this problem. However, again our data has units and a physical meaning we want to preserve.</p>
<p>As an alternative we can scale the parameter values themselves both to clip the values to a physical range and to scale the gradients and hopefully improve the convergence of the optimization step. We do that here, in a way that should be mostly invisible to the training algorithms.</p>
<div class="fragment"><div class="line"><span class="comment"># Scale to get better performance</span></div>
<div class="line">A_scaler = reparametrization.RangeRescale(</div>
<div class="line">    torch.tensor(-12.0, device=device), torch.tensor(-4.0, device=device)</div>
<div class="line">)</div>
<div class="line">B_scaler = reparametrization.RangeRescale(</div>
<div class="line">    torch.tensor(-1.0, device=device), torch.tensor(-0.5, device=device)</div>
<div class="line">)</div>
<div class="line">C_scaler = reparametrization.RangeRescale(</div>
<div class="line">    torch.tensor(-8.0, device=device), torch.tensor(-3.0, device=device)</div>
<div class="line">)</div>
<div class="line">R_scaler = reparametrization.RangeRescale(</div>
<div class="line">    torch.tensor([0.0, 0.0, 0.0, 0.0], device=device),</div>
<div class="line">    torch.tensor([500.0, 500.0, 500.0, 500.0], device=device),</div>
<div class="line">)</div>
<div class="line">d_scaler = reparametrization.RangeRescale(</div>
<div class="line">    torch.tensor([0.01, 0.01, 0.01, 0.01], device=device),</div>
<div class="line">    torch.tensor([50.0, 50.0, 50.0, 50.0], device=device),</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">model_reparameterizer = reparametrization.Reparameterizer(</div>
<div class="line">    {</div>
<div class="line">        <span class="stringliteral">&quot;discrete_equations.A_value&quot;</span>: A_scaler,</div>
<div class="line">        <span class="stringliteral">&quot;discrete_equations.B_value&quot;</span>: B_scaler,</div>
<div class="line">        <span class="stringliteral">&quot;discrete_equations.C_value&quot;</span>: C_scaler,</div>
<div class="line">        <span class="stringliteral">&quot;discrete_equations.R_Y&quot;</span>: R_scaler,</div>
<div class="line">        <span class="stringliteral">&quot;discrete_equations.d_Y&quot;</span>: d_scaler,</div>
<div class="line">    },</div>
<div class="line">    error_not_provided=<span class="keyword">True</span>,</div>
<div class="line">)</div>
<div class="line">model_reparameterizer(model)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md199"></a>
Convert the model to a hierarchical statistical model</h1>
<p>We now convert the model from deterministic to a hierarchnical statistical model. We use the <span class="tt">pyzag</span> mapper functionality to convert the model with two levels. The top level statistics are common to the entire data set, i.e. all tests in the synthetic data, while the lower-level distributions provide statistics for each individual sample of material. Each parameter is converted to a prior with the initial mean value selected based on the (random) deterministic values assigned above. We also provide a prior on the level of white noise included in the experimental measurements.</p>
<div class="fragment"><div class="line">mapper = stochastic.MapNormal(prior_cov)</div>
<div class="line">hsmodel = stochastic.HierarchicalStatisticalModel(</div>
<div class="line">    model, mapper, torch.tensor(prior_noise_scale, device=device)</div>
<div class="line">)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md200"></a>
Sample the prior model to compare to the repeated test</h1>
<p>This is just for visualization, to see how far away our prior model starts from the true distribution</p>
<div class="fragment"><div class="line">nreps = 5</div>
<div class="line">predict = Predictive(hsmodel, num_samples=nreps)</div>
<div class="line"><span class="keyword">with</span> torch.no_grad():</div>
<div class="line">    untrained_single = predict(</div>
<div class="line">        single_times[:, :nbatch], single_temperatures[:, :nbatch], single_loading[:, :nbatch]</div>
<div class="line">    )[<span class="stringliteral">&quot;obs&quot;</span>]</div>
</div><!-- fragment --><div class="fragment"><div class="line">flat_untrained = untrained_single.transpose(0, 1).reshape(ntime, nreps * nbatch, 1)</div>
<div class="line">plt.figure()</div>
<div class="line">plt.plot(</div>
<div class="line">    single_loading[:, 0, 0].cpu(),</div>
<div class="line">    torch.mean(flat_untrained, dim=1)[:, 0].cpu(),</div>
<div class="line">    ls=<span class="stringliteral">&quot;-&quot;</span>,</div>
<div class="line">    color=<span class="stringliteral">&quot;k&quot;</span>,</div>
<div class="line">    lw=4,</div>
<div class="line">    label=<span class="stringliteral">&quot;Prior mean&quot;</span>,</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">p = 0.05</div>
<div class="line">n_lb = int(p * nreps * nbatch)</div>
<div class="line">n_ub = int((1 - p) * nreps * nbatch)</div>
<div class="line">plt.fill_between(</div>
<div class="line">    single_loading[:, 0, 0].cpu(),</div>
<div class="line">    torch.kthvalue(flat_untrained, n_lb, dim=1)[0][:, 0].cpu(),</div>
<div class="line">    torch.kthvalue(flat_untrained, n_ub, dim=1)[0][:, 0].cpu(),</div>
<div class="line">    color=<span class="stringliteral">&quot;tab:blue&quot;</span>,</div>
<div class="line">    alpha=0.8,</div>
<div class="line">    label=<span class="stringliteral">&quot;90% prediction&quot;</span>,</div>
<div class="line">)</div>
<div class="line">plt.plot(</div>
<div class="line">    single_loading.cpu()[..., 0],</div>
<div class="line">    single_data[..., 0].cpu(),</div>
<div class="line">    color=<span class="stringliteral">&quot;k&quot;</span>,</div>
<div class="line">    lw=0.3,</div>
<div class="line">    label=<span class="stringliteral">&quot;Data&quot;</span>,</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">handles, labels = plt.gca().get_legend_handles_labels()</div>
<div class="line"> </div>
<div class="line">plt.xlabel(<span class="stringliteral">&quot;Strain&quot;</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&quot;Stress&quot;</span>)</div>
<div class="line">plt.legend(handles[:3], labels[:3], loc=<span class="stringliteral">&quot;best&quot;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --><div class="image">
<img src="statistical_32_0.png" alt=""/>
<div class="caption">
png</div></div>
    <h2 class="doxsection"><a class="anchor" id="autotoc_md201"></a>
Setup a guide and training hyperparameters</h2>
<p>Use <span class="tt">AutoDelta</span> to get a MAP estimate of the parameters.</p>
<p>Setup the SVI problem and the usual sorts of training hyperparameters</p>
<div class="fragment"><div class="line">guide = pyro.infer.autoguide.guides.AutoDelta(hsmodel)</div>
<div class="line">lr = 1.0e-3</div>
<div class="line">niter = 150</div>
<div class="line">num_samples = 1</div>
<div class="line"> </div>
<div class="line">optimizer = pyro.optim.ClippedAdam({<span class="stringliteral">&quot;lr&quot;</span>: lr})</div>
<div class="line">loss = Trace_ELBO(num_particles=num_samples)</div>
<div class="line">svi = SVI(hsmodel, guide, optimizer, loss=loss)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="autotoc_md202"></a>
Run the training loop</h1>
<div class="fragment"><div class="line">titer = tqdm.tqdm(</div>
<div class="line">    range(niter),</div>
<div class="line">    bar_format=<span class="stringliteral">&quot;{desc}{percentage:3.0f}%|{bar}|{n_fmt}/{total_fmt}{postfix}&quot;</span>,</div>
<div class="line">)</div>
<div class="line">titer.set_description(<span class="stringliteral">&quot;Loss:&quot;</span>)</div>
<div class="line">loss_history = []</div>
<div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> titer:</div>
<div class="line">    closs = svi.step(time, temperature, loading, results=data)</div>
<div class="line">    loss_history.append(closs)</div>
<div class="line">    titer.set_description(<span class="stringliteral">&quot;Loss: %3.2e&quot;</span> % closs)</div>
<div class="line"> </div>
<div class="line">plt.figure()</div>
<div class="line">plt.plot(loss_history, label=<span class="stringliteral">&quot;Training&quot;</span>)</div>
<div class="line">plt.xlabel(<span class="stringliteral">&quot;Iteration&quot;</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&quot;ELBO&quot;</span>)</div>
<div class="line">plt.legend(loc=<span class="stringliteral">&quot;best&quot;</span>)</div>
<div class="line">plt.title(<span class="stringliteral">&quot;Training loss&quot;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --> <pre class="fragment">Loss: 1.52e+04: 100%|██████████|150/150
</pre><div class="image">
<img src="statistical_36_1.png" alt=""/>
<div class="caption">
png</div></div>
    <h1 class="doxsection"><a class="anchor" id="autotoc_md203"></a>
Go back and sample our repeated case</h1>
<div class="fragment"><div class="line">nreps = 5</div>
<div class="line">predict = Predictive(hsmodel, guide=guide, num_samples=nreps)</div>
<div class="line"><span class="keyword">with</span> torch.no_grad():</div>
<div class="line">    trained_single = predict(</div>
<div class="line">        single_times[:, :nbatch], single_temperatures[:, :nbatch], single_loading[:, :nbatch]</div>
<div class="line">    )[<span class="stringliteral">&quot;obs&quot;</span>]</div>
</div><!-- fragment --><div class="fragment"><div class="line">flat_trained = trained_single.transpose(0, 1).reshape(ntime, nreps * nbatch, 1)</div>
<div class="line">plt.figure()</div>
<div class="line">plt.plot(</div>
<div class="line">    single_loading[:, 0, 0].cpu(),</div>
<div class="line">    torch.mean(flat_trained, dim=1)[:, 0].cpu(),</div>
<div class="line">    ls=<span class="stringliteral">&quot;-&quot;</span>,</div>
<div class="line">    color=<span class="stringliteral">&quot;k&quot;</span>,</div>
<div class="line">    lw=4,</div>
<div class="line">    label=<span class="stringliteral">&quot;Trained mean&quot;</span>,</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">p = 0.05</div>
<div class="line">n_lb = int(p * nreps * nbatch)</div>
<div class="line">n_ub = int((1 - p) * nreps * nbatch)</div>
<div class="line">plt.fill_between(</div>
<div class="line">    single_loading[:, 0, 0].cpu(),</div>
<div class="line">    torch.kthvalue(flat_trained, n_lb, dim=1)[0][:, 0].cpu(),</div>
<div class="line">    torch.kthvalue(flat_trained, n_ub, dim=1)[0][:, 0].cpu(),</div>
<div class="line">    color=<span class="stringliteral">&quot;tab:blue&quot;</span>,</div>
<div class="line">    alpha=0.8,</div>
<div class="line">    label=<span class="stringliteral">&quot;90% prediction&quot;</span>,</div>
<div class="line">)</div>
<div class="line">plt.plot(single_loading.cpu()[..., 0], single_data[..., 0].cpu(), color=<span class="stringliteral">&quot;k&quot;</span>, lw=0.3, label=<span class="stringliteral">&quot;Data&quot;</span>)</div>
<div class="line"> </div>
<div class="line">handles, labels = plt.gca().get_legend_handles_labels()</div>
<div class="line"> </div>
<div class="line">plt.xlabel(<span class="stringliteral">&quot;Strain&quot;</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&quot;Stress&quot;</span>)</div>
<div class="line">plt.legend(handles[:3], labels[:3], loc=<span class="stringliteral">&quot;best&quot;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --><div class="image">
<img src="statistical_39_0.png" alt=""/>
<div class="caption">
png</div></div>
    <h1 class="doxsection"><a class="anchor" id="autotoc_md204"></a>
Compare priors, true posteriors, and inferred posteriors</h1>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> _, _, n <span class="keywordflow">in</span> hsmodel.bot:</div>
<div class="line">    simple_name = <span class="stringliteral">&quot;.&quot;</span>.join([n.split(<span class="stringliteral">&quot;.&quot;</span>)[i] <span class="keywordflow">for</span> i <span class="keywordflow">in</span> [0, 2]])</div>
<div class="line">    scaler = model_reparameterizer.map_dict[simple_name]</div>
<div class="line">    prior_loc = guess_parameter_values[simple_name]</div>
<div class="line">    prior_scale = prior_cov * torch.abs(prior_loc)</div>
<div class="line">    actual_loc = actual_parameter_values[simple_name]</div>
<div class="line">    actual_scale = actual_cov * torch.abs(actual_loc)</div>
<div class="line"> </div>
<div class="line">    posterior_loc = scaler.forward(pyro.param(<span class="stringliteral">&quot;AutoDelta.&quot;</span> + n + <span class="stringliteral">&quot;_loc&quot;</span>).detach())</div>
<div class="line">    posterior_scale = scaler.forward_std_dev(pyro.param(<span class="stringliteral">&quot;AutoDelta.&quot;</span> + n + <span class="stringliteral">&quot;_scale&quot;</span>).detach())</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> posterior_loc.dim() == 0:</div>
<div class="line">        x_range = torch.linspace(</div>
<div class="line">            posterior_loc - 20 * posterior_scale,</div>
<div class="line">            posterior_loc + 20 * posterior_scale,</div>
<div class="line">            200,</div>
<div class="line">            device=device,</div>
<div class="line">        )</div>
<div class="line">        y_posterior = dist.Normal(posterior_loc.cpu(), posterior_scale).log_prob(x_range).exp()</div>
<div class="line">        y_actual = dist.Normal(actual_loc, actual_scale).log_prob(x_range).exp()</div>
<div class="line">        y_prior = dist.Normal(prior_loc, prior_scale).log_prob(x_range).exp()</div>
<div class="line">        plt.figure()</div>
<div class="line">        plt.fill_between(x_range.cpu(), y_actual.cpu(), label=<span class="stringliteral">&quot;Actual&quot;</span>, alpha=0.5)</div>
<div class="line">        plt.fill_between(x_range.cpu(), y_posterior.cpu(), label=<span class="stringliteral">&quot;Posterior&quot;</span>, alpha=0.5)</div>
<div class="line">        plt.fill_between(x_range.cpu(), y_prior.cpu(), label=<span class="stringliteral">&quot;Prior&quot;</span>, alpha=0.5)</div>
<div class="line">        plt.legend(loc=<span class="stringliteral">&quot;best&quot;</span>)</div>
<div class="line">        plt.title(simple_name)</div>
<div class="line">    <span class="keywordflow">else</span>:</div>
<div class="line">        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(posterior_loc.shape[0]):</div>
<div class="line">            x_range = torch.linspace(</div>
<div class="line">                posterior_loc[i] - 20 * posterior_scale[i],</div>
<div class="line">                posterior_loc[i] + 20 * posterior_scale[i],</div>
<div class="line">                200,</div>
<div class="line">                device=device,</div>
<div class="line">            )</div>
<div class="line">            y_posterior = dist.Normal(posterior_loc[i], posterior_scale[i]).log_prob(x_range).exp()</div>
<div class="line">            y_actual = dist.Normal(actual_loc[i], actual_scale[i]).log_prob(x_range).exp()</div>
<div class="line">            y_prior = dist.Normal(prior_loc[i], prior_scale[i]).log_prob(x_range).exp()</div>
<div class="line">            plt.figure()</div>
<div class="line">            plt.fill_between(x_range.cpu(), y_actual.cpu(), label=<span class="stringliteral">&quot;Actual&quot;</span>, alpha=0.5)</div>
<div class="line">            plt.fill_between(x_range.cpu(), y_posterior.cpu(), label=<span class="stringliteral">&quot;Posterior&quot;</span>, alpha=0.5)</div>
<div class="line">            plt.fill_between(x_range.cpu(), y_prior.cpu(), label=<span class="stringliteral">&quot;Prior&quot;</span>, alpha=0.5)</div>
<div class="line">            plt.legend(loc=<span class="stringliteral">&quot;best&quot;</span>)</div>
<div class="line">            plt.title(simple_name + <span class="stringliteral">&quot; component %i&quot;</span> % i)</div>
</div><!-- fragment --><div class="image">
<img src="statistical_41_0.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_1.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_2.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_3.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_4.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_5.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_6.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_7.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_8.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_9.png" alt=""/>
<div class="caption">
png</div></div>
    <div class="image">
<img src="statistical_41_10.png" alt=""/>
<div class="caption">
png</div></div>
     </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.16.1 </li>
  </ul>
</div>
</body>
</html>
