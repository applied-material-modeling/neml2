{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic material model learning\n",
    "\n",
    "This is a complete example of using the pyzag bindings to NEML2 to calibrated a material model against experimental data.  `demo_model.i` defines the constitutive model, which is a structural material model describing the evolution of strain and stress in the material under mechanical load.  The particular demonstration is a fairly complex model where the material responds differently as a function of both temperature and strain rate.  The model also reduces the full 3D form of the constitutive model to 1D, where the final model is driven by an axial strain and all the other stress components are zero.\n",
    "\n",
    "In this example we:\n",
    "1. Load the model in from file and wrap it for use in pyzag\n",
    "2. Setup a grid of \"experimental\" conditions spanning several strain rates and temperatures\n",
    "3. Replace the original model parameters with samples from a narrow normal distrubtion, centered on the orignial model mean, and run the model over the experimental conditions.  This then becomes our synthetic input data.\n",
    "4. Replace the original model parameters with random initial guesses (taken from a very wide normal distribution around the true values).\n",
    "5. Setup the model for training with gradient-descent methods by scaling the model parameters and resulting gradient values.\n",
    "6. Train the model against the synthetic experimental data.\n",
    "7. Plot the results and print the trained parameter values, to see how close we can come to the true values.\n",
    "\n",
    "The accuracy of the final model and the trained parameter values is heavily dependent on the choice of the normal distributions for the synthetic data and the initial parameter guesses.  For narrow distributions for both the model can exactly recover the original parameter values.  For wider distributions the trained model will not be exact, but will still accurately capture the mean of the synthetic tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import neml2\n",
    "from neml2.pyzag import interface\n",
    "from pyzag import nonlinear, reparametrization, chunktime\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters related to *how* we train the model\n",
    "\n",
    "Choose which device to use.  The `nchunk` parameter controls the time integration in `pyzag`.  `pyzag` can vectorize the time integration itself, providing a larger bandwidth to the compute device.  This helps speed up the calculation, particularly when running on a GPU.  The optimal value will depend on your compute device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "nchunk = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the synthetic experimental conditions\n",
    "\n",
    "Setup the loading conditions for the \"experiments\" we're going to run.  These will span several strain rates (`nrate`) and temperatures (`ntemperature`).  Overall, we'll run `nbatch` experiments.  Also setup the maximum strain to pull the material through `max_strain` and the number of time steps we're going to use for integration `ntime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrate = 5\n",
    "ntemperature = 5\n",
    "nbatch = nrate * ntemperature\n",
    "max_strain = 0.25\n",
    "ntime = 100\n",
    "rates = torch.logspace(-6,0,nrate, device = device)\n",
    "temperatures = torch.linspace(310.0,1190.0,ntemperature, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the variability in the synthetic data and for our initial guess at the parameters\n",
    "\n",
    "These control the variability in the synthetic data (`actual_cov`) and the variability of the initial guess at the parameter values (`guess_cov`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_cov = 0.025\n",
    "guess_cov = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the actual model\n",
    "\n",
    "This class is a thin wrapper around the underlying `pyzag` wrapper for NEML2.  All it does is take the input conditions (time, temperature, and strain), combine them into a single tensor, call the `pyzag` wrapper, and return the stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveStrain(torch.nn.Module):\n",
    "    \"\"\"Just integrate the model through some strain history\n",
    "\n",
    "    Args:\n",
    "        discrete_equations: the pyzag wrapped model\n",
    "        nchunk (int): number of vectorized time steps\n",
    "        rtol (float): relative tolerance to use for Newton's method during time integration\n",
    "        atol (float): absolute tolerance to use for Newton's method during time integration\n",
    "    \"\"\"\n",
    "    def __init__(self, discrete_equations, nchunk = 1, rtol = 1.0e-6, atol = 1.0e-4):\n",
    "        super().__init__()\n",
    "        self.discrete_equations = discrete_equations\n",
    "        self.nchunk = nchunk\n",
    "        self.cached_solution = None\n",
    "        self.rtol = rtol\n",
    "        self.atol = atol\n",
    "\n",
    "    def forward(self, time, temperature, loading, cache = False):\n",
    "        \"\"\"Integrate through some time/temperature/strain history and return stress\n",
    "        Args:\n",
    "            time (torch.tensor): batched times\n",
    "            temperature (torch.tensor): batched temperatures\n",
    "            loading (torch.tensor): loading conditions, which are the input strain in the first base index and then the stress (zero) in the remainder\n",
    "\n",
    "        Keyword Args:\n",
    "            cache (bool): if true, cache the solution and use it as a predictor for the next call.  \n",
    "                This heuristic can speed things up during inference where the model is called repeatedly with similar parameter values.\n",
    "        \"\"\"\n",
    "        if cache and self.cached_solution is not None:\n",
    "            solver = nonlinear.RecursiveNonlinearEquationSolver(\n",
    "                self.discrete_equations,\n",
    "                step_generator=nonlinear.StepGenerator(self.nchunk),\n",
    "                predictor=nonlinear.FullTrajectoryPredictor(self.cached_solution), nonlinear_solver = chunktime.ChunkNewtonRaphson(rtol = self.rtol, atol = self.atol)\n",
    "            )\n",
    "        else:\n",
    "            solver = nonlinear.RecursiveNonlinearEquationSolver(\n",
    "                self.discrete_equations, step_generator=nonlinear.StepGenerator(self.nchunk),\n",
    "                predictor=nonlinear.PreviousStepsPredictor(), nonlinear_solver = chunktime.ChunkNewtonRaphson(rtol = self.rtol, atol = self.atol)\n",
    "            )\n",
    "\n",
    "        # We could pass this in as input, but it's easy enough to do here\n",
    "        control = torch.zeros_like(loading)\n",
    "        control[...,1:] = 1.0\n",
    "\n",
    "        # Setup\n",
    "        forces = self.discrete_equations.collect_forces({'t': time, 'T': temperature, 'fixed_values': loading, 'control': control})\n",
    "        state0 = torch.zeros(forces.shape[1:-1] + (self.discrete_equations.nstate,), device = forces.device)\n",
    "\n",
    "        result =  nonlinear.solve_adjoint(\n",
    "            solver, state0, len(forces), forces\n",
    "        )\n",
    "\n",
    "        if cache:\n",
    "            self.cached_solution = result.detach().clone()\n",
    "\n",
    "        return result[...,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually setup the model\n",
    "\n",
    "Load the NEML model from disk, wrap it in both the `pyzag` wrapper and our thin wrapper class above.  Exclude some of the model parameters we don't want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SolveStrain(interface.NEML2PyzagModel(neml2.load_model(\"demo_model.i\", \"implicit_rate\"), exclude_parameters = [\"elasticity.E\", \"elasticity.nu\",\"R.X\", \"d.X\", \"mu.X\", \"mu.Y\",\"yield_zero.sy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the input tensors\n",
    "\n",
    "Actually setup the full input tensors based on the parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.zeros((ntime, nrate, ntemperature, 1), device = device)\n",
    "loading = torch.zeros((ntime, nrate, ntemperature, 6), device = device)\n",
    "temperature = torch.zeros((ntime, nrate, ntemperature, 1), device = device)\n",
    "for i, rate in enumerate(rates):\n",
    "    time[:, i] = torch.linspace(0, max_strain / rate, ntime, device = device)[:, None, None]\n",
    "loading[...,0] = torch.linspace(0, max_strain, ntime, device = device)[:, None, None]\n",
    "for i, T in enumerate(temperatures):\n",
    "    temperature[:, :, i] = T\n",
    "time = time.reshape((ntime, -1, 1))\n",
    "temperature = temperature.reshape((ntime, -1, 1))\n",
    "loading = loading.reshape((ntime, -1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the model parameters with random values\n",
    "\n",
    "Sampled from a normal distribution controlled by the `actual_cov` parameter.\n",
    "\n",
    "This controls the randomness in the input synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with samples from normal\n",
    "actual_parameter_values = {}\n",
    "for n,p in model.named_parameters():\n",
    "    actual_parameter_values[n] = p.data.detach().clone().cpu()\n",
    "    ndist = dist.Normal(p.data, torch.abs(p.data) * actual_cov).expand((nbatch,)+p.shape)\n",
    "    p.data = ndist.sample().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model to generate the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data = model(time, temperature, loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loading.cpu()[...,0], data[...,0].cpu())\n",
    "plt.xlabel(\"Strain (mm/mm)\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Input data -- all conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model for training\n",
    "\n",
    "Replace the parameter values with random initial guesses, with variability controlled by the `guess_cov` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now replace our original parameter with random values over a range\n",
    "guess_parameter_values = {}\n",
    "for n, p in model.named_parameters():\n",
    "    p.data = torch.normal(actual_parameter_values[n], torch.abs(actual_parameter_values[n])*guess_cov).to(device)\n",
    "    guess_parameter_values[n] = p.data.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the model parameters\n",
    "\n",
    "Our material model parameters have units.  In general, the parameter values will have different magnitudes from each other, which affects the scale of the gradients.  Unbalanced gradients in turn affect the convergence of gradient descent optimization methods.\n",
    "\n",
    "Typically we'd scale the training data to fix this problem.  However, again our data has units and a physical meaning we want to preserve.\n",
    "\n",
    "As an alternative we can scale the parameter values themselves both to clip the values to a physical range and to scale the gradients and hopefully improve the convergence of the optimization step.  We do that here, in a way that should be mostly invisible to the training algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to get better performance\n",
    "A_scaler = reparametrization.RangeRescale(torch.tensor(-12.0, device = device), torch.tensor(-4.0, device = device))\n",
    "B_scaler = reparametrization.RangeRescale(\n",
    "    torch.tensor(-1.0, device = device), torch.tensor(-0.5, device = device)\n",
    ")\n",
    "C_scaler = reparametrization.RangeRescale(torch.tensor(-8.0, device = device), torch.tensor(-3.0, device = device))\n",
    "R_scaler = reparametrization.RangeRescale(\n",
    "    torch.tensor([0.0,0.0,0.0,0.0], device=device), torch.tensor([500.0,500.0,500.0,500.0], device=device)\n",
    ")\n",
    "d_scaler = reparametrization.RangeRescale(\n",
    "    torch.tensor([0.01, 0.01, 0.01, 0.01], device=device),\n",
    "    torch.tensor([50.0, 50.0, 50.0, 50.0], device=device),\n",
    ")\n",
    "\n",
    "model_reparameterizer = reparametrization.Reparameterizer(\n",
    "    {\"discrete_equations.A_value\": A_scaler,\n",
    "     \"discrete_equations.B_value\": B_scaler,\n",
    "     \"discrete_equations.C_value\": C_scaler,\n",
    "     \"discrete_equations.R_Y\": R_scaler,\n",
    "     \"discrete_equations.d_Y\": d_scaler}, error_not_provided=True)\n",
    "model_reparameterizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with the initial parameter values\n",
    "\n",
    "Just to see how far away from the training data we starting from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the initial results so we know where we are starting from\n",
    "with torch.no_grad():\n",
    "    initial_results = model(time, temperature, loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loading.cpu()[..., 0], data.cpu(), 'k-')\n",
    "plt.plot(loading.cpu()[..., 0], initial_results.cpu(), 'k--')\n",
    "plt.xlabel(\"Strain (mm/mm)\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Initial comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model against the synthetic data\n",
    "\n",
    "\n",
    "Apply a fairly standard training algorithm to adjust the model parameters to better match the synthetic data.  Plot the loss versus iteration data from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 200\n",
    "lr = 5.0e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "titer = tqdm.tqdm(range(niter))\n",
    "titer.set_description(\"Loss:\")\n",
    "loss_history = []\n",
    "for i in titer:\n",
    "    optimizer.zero_grad()\n",
    "    res = model(time, temperature, loading, cache = True)\n",
    "    loss = loss_fn(res, data)\n",
    "    loss.backward()\n",
    "    loss_history.append(loss.detach().clone().cpu())\n",
    "    titer.set_description(\"Loss: %3.2e\" % loss_history[-1])\n",
    "    optimizer.step()\n",
    "\n",
    "plt.loglog(loss_history, label=\"Training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the train model predictions\n",
    "\n",
    "See how accurately the trained model recovers the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loading.cpu()[..., 0], data.cpu(), \"k-\")\n",
    "plt.plot(loading.cpu()[..., 0], res.detach().cpu(), \"k--\")\n",
    "plt.xlabel(\"Strain (mm/mm)\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Final comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the trained model coefficients\n",
    "\n",
    "These may vary sigifnicantly from the actual mean values, particularly if you used a large variance when sampling the parameters to generate the synetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The results:\")\n",
    "for n, p in model.discrete_equations.named_parameters():\n",
    "    nice_name = n.split('.')[-2]\n",
    "    ref_name = \"discrete_equations.\" + nice_name\n",
    "    scaler = model_reparameterizer.map_dict[ref_name]\n",
    "    print(nice_name)\n",
    "    print(\"\\tInitial: \\t\" + str(guess_parameter_values[ref_name].cpu()))\n",
    "    print(\"\\tOptimized: \\t\" + str(scaler(p.data).cpu()))\n",
    "    print(\"\\tTrue value: \\t\" + str(actual_parameter_values[ref_name].cpu()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
